{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "\n",
    "# Part 1. Mini Project\n",
    "\n",
    "### 비즈니스 문제를 해결하기 위해 데이터로 어떻게 접근, 해결할지 생각해보는 문제\n",
    "    - Q. 1~3 문제 : 잔존율 증가를 위한 비즈니스 문제해결 \n",
    "    - Q. 4~6 문제 : 금융권 VIP 고객의 연간 소비금액을 예측하기 위한 모델 구축 작업\n",
    "    - Q. 7~8 문제 : 유저 세분화 및 그룹핑을 위한 데이터 분석 작업\n",
    "\n",
    "\n",
    "### 머신러닝의 학습과정 및 활용에 대한 이해를 확인하는 문제\n",
    "    - Q. 9 문제 : 손실함수의 필요성과 개념에 대한 이해\n",
    "    - Q. 10~11 문제 : Gradient Descent의 과정과 SGD, MGD 처리 방식의 이해\n",
    "    - Q. 12-13 문제 : 오버피팅 개념 및 해결방법에 대한 이해\n",
    "    - Q. 14-15 문제 : CNN 및 RNN 의 동작원리에 대한 이해와 활용\n",
    "\n",
    "\n",
    "- 작성자: 송훈화 감수자\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q. 1~3. 잔존율 증가를 위한 비즈니스 문제해결 방안 마련\n",
    "\n",
    "### 배경\n",
    "다음과 같은 비즈니스 문제가 있다고 하자. \n",
    "현재 앱서비스의 잔존율이 정체되어 있는 상황이며 재이용자의 증가가 필수적인 상황이다. 여기서 잔존율이란 서비스를 이용하던 기존 유저가 시간이 흘러도 지속적으로 이용하고 있는 정도를 의미한다. \n",
    "\n",
    "\n",
    "### 목표\n",
    "분석가에게 주어진 역할은 사내에 수집된 데이터를 추출해 잔존율을 높일 수 있는 방안을 유관팀에 공유하는 것이다. 유관팀은 개별 유저별로 잔존 여부를 예측할 수 있다면, 이를 근거로 개인화된 타깃팅을 진행할 수 있을 것이다. 분석가는 데이터를 근거로 유저들의 잔존 여부를 예측할 수 있는 모델을 구축하고자 한다. 즉 분석 목표는\n",
    "   - 잔존 vs 비잔존 그룹간의 유저 행동 패턴을 이해하고,\n",
    "   - 개별 유저의 잔존 여부(Y/N)를 예측할 수 있는 모델을 만드는 것이다.\n",
    "\n",
    "위에 주어진 목표를 달성하기 위해 어떤 접근방법을 활용해야할지 각 단계별로 기술해보자.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 수집: 어떤 데이터를 수집/추출할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "어떠한 이유때문에 유저들이 이탈하는지에 대한 가설을 세우고. 이에대해서 알아보기 위해 데이터들을 비교해야한다. 이러한 과정에서 변수들간의 비교그래프를 그려 볼 수 있고 변수들간의 상관관계를 따져볼 수 있다.\n",
    "\n",
    "잔존율은 정체되고 재이용자의 수를 증가시켜야 하는 상황이므로 잔존그룹과 비잔존 그룹의 부분 데이터를 선택하고 선택한 변수들을 병합하여 파생변수를 만들어야한다.\n",
    "\n",
    "이 후, 가설에 대한 검증을 하고 가설이 검증되거나 검증될 확률이 높은 결과를 도출해낸다면 그에대한 데이터를 분석해야한다. 만약 유의미한 결과를 얻지 못한다면 다시 가설을 세우고 다른 변수를 추가하는 방법을 쓸 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 이번 데이터 분석의 목표는 잔존그룹과 비잔존 그룹간의 차이를 파악하고 개별 유저의 잔존여부를 예측할 수 있는 모델을 만드는 것입니다.\n",
    "   - 즉 개별 유저의 행동패턴을 파악해 잔존여부를 예측하고자 하는 것입니다.\n",
    "   - 이를 위해선 개별 유저의 행동패턴에 대한 데이터들이 필요하겠지요.\n",
    "     - 성별, 나이, 사는 지역, 하루 평균 앱 접속 횟수, 1주간 방문횟수 등\n",
    "     - 유니크한 정보임을 알기위해 유저 아이디나 이메일도 필요하겠네요\n",
    "   - 위와 같은 문제에서는 어떤 앱을 자신이 사용하고 있는지 가정하고 투영해보면 좀 더 쉽게 접근하실 수 있으리라 생각됩니다.\n",
    "   - 서비스 회사에서 어떤 로그를 가지고 있는지 예상하기가 힘드시다면 kaggle을 참고하시는 것을 추천드립니다.\n",
    "   - kaggle에는 실제 회사들이 자신들이 가지고 있는 데이터 일부를 제공하기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 탐색적 데이터 분석: 유저의 행동패턴 이해를 위해 데이터 탐색을 어떻게 할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "유저들의 행동패턴 이해를 위해서 잔존유저, 비잔존 유저들의 각각의 변수들을 뽑아서 변수들간의 관계나 특성을 분석하고 기술 통계량 계산과 여러가지 그래프를 활용해 볼 수 있다.\n",
    "이 데이터들에는 수 많은 변수들이 존재할 것이므로 이에대한 데이터를 인식 가능한 수준으로 요약해야한다. 정해진 알고리즘을 이용해서 데이터 정보를 찾아낼 수 있는데 이 알고리즘을 이용해서 데이터들의 변수와 관측치 간 관계를 확인해 볼 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 유저의 행동 패턴을 찾기 위해 어떻게 분석할 것인지를 물어보는 문제입니다.\n",
    "     - 조금 더 분명히 하자면 잔존그룹과 비잔존그룹의 다른 행동패턴을 찾아내는 것이지요\n",
    "   - 예를 들면 유저별 방문주기, 방문대비구매빈도를 히스토그램으로 그려 비교할 수 있겠네요\n",
    "     - 추가적으로 성별같은 경우 Cross Table을 사용해 분석할 수 있겠습니다.\n",
    "   - 본 데이터 분석의 목적은 \"개별유저의 잔존여부 예측\" 이라는 것을 잊지 말아주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모형 적합: 어떤 예측모형을 이용해 잔존 여부를 예측할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "향후 잔존율에 대한 예측을 하는것이기 때문에 로지스틱 회귀분석을 이용하여\n",
    "종속변수와 독립변수를 설정하고 이에대한 예측값을 알아 볼 수 있다. 이 외에도 랜덤\n",
    "포레스트 기법이나 SVM 을 잔존여부 예측에 이용할 수도 있다.\n",
    "\n",
    "모델마다 성능이 다르게 나올 것이기 때문에 가장 적합한 모델을 사용해야하며 이\n",
    "과정에서 목적에 맞는 feature 를 합리적으로 만들어야한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "  - 예측 모형은 주로 타깃 데이터(y 데이터)의 형태에 따라 달라집니다.\n",
    "  - 위의 모델은 개별 유저의 잔존 여부를 나타내므로 Discrete합니다.\n",
    "    - 반대는 continuous하겠죠?\n",
    "  - y값이 존재하고 연속되지 않는, 이산형변수에 대한 예측인 것이지요.\n",
    "  - 따라서, 타깃데이터가 존재하는 지도학습 중에서 이산형변수에 대한 예측이 가능한 분류모델이 적합할 것입니다.\n",
    "  - 분류 모델에는 Logistic Regression, Decision Tree 등이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q. 4~6. 금융권 VIP 고객의 연간 소비금액을 예측하기 위한 모델 구축 작업\n",
    "\n",
    "### 배경 및 목표\n",
    "금융권의 한 기업에서 VIP 고객들의 연간 소비금액(단위: 원)을 예측하기 위한 모델을 만들고 있다. 영업팀은 이 예측모델을 새로운 고객관리시스템에 도입하고자 준비하고 있다. 분석가의 목표는 기존 VIP 고객들의 소비금액을 기반으로 새로운 VIP 고객의 연간 소비금액(단위: 원)을 예측하는 것이다.\n",
    "\n",
    "### 데이터셋\n",
    "분석가에게 주어진 데이터셋의 컬럼은 아래와 같다. \n",
    "\n",
    "- 고객아이디(숫자형)\n",
    "- 연봉(숫자형)\n",
    "- 주소(문자) \n",
    "- 연간 소비금액 (숫자형, 단위: 원)\n",
    "- 성별(문자)\n",
    "- 계좌 잔고금액(숫자형, 단위: 원)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 입력 데이터(X, features)로 적절한 변수와 타깃 데이터(y, target, label)로 적절한 변수는 각각 무엇일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "입력데이터 X 들로 타깃 데이터 Y 를 예측하고 싶은 문제이기 때문에 Y 는 연간 소비금액이 되어야 한다.\n",
    "\n",
    "입력 데이터는 고객의 연봉, 계좌 잔고금액을 사용할 수 있다.\n",
    "\n",
    "고객아이디, 주소, 성별은 입력데이터로 부적절하다고 볼 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 이번엔 데이터셋이 주어졌으며, 어떤 데이터를 통해 어떤 타깃을 예측할 것인지에 대한 문제입니다.\n",
    "   - 입력 데이터는 타깃데이터와 어떤 연관성이 존재할 수 있는 데이터를 선택해야 예측을 잘 할 것입니다.\n",
    "   - 목표에서 새로운 VIP의 연간 소비금액에 대한 예측을 원한다고 하였으니 타깃 데이터는 연간소비금액이 되겠네요\n",
    "     - 회귀에서의 타깃데이터는 하나입니다.\n",
    "   - 입력 데이터로는 연봉, 주소, 계좌잔고금액, 성별이 예측하는 데 쓰일 수 있겠습니다.\n",
    "     - 성별이나 주소는 연관성을 찾아볼 수 없다고 말하실 수도 있지만\n",
    "     - 동네에 따라, 성별에 따라 소비가 많고 적고가 달라질 수도 있으니까요\n",
    "   - 고객 아이디의 경우 단순나열에 불과하기에 연간 소비금액과 연관성이 있다곤 힘들어 입력데이터로는 적합하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 지도학습(회귀), 지도학습(분류), 비지도학습, 강화학습 중에 어떤 모델을 적용할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "입력 데이터 X 와 타겟값 Y 를 알고있는 데이터를 학습하여 이들의 관계를 모델링 하는\n",
    "학습방법은 지도학습(회귀)이다.\n",
    "\n",
    "이 문제에서는 비지도학습처럼 타겟값 Y 가 없는 입력데이터 X 만을 학습하는 것이\n",
    "아니기 때문에 지도학습을 사용해야한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 생성된 모델이 학습 데이터에서는 성능이 높았으나 테스트 데이터에서 성능이 낮았다. 추정되는 이유는 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test set 에서 실제로는 타겟값이 있지만 없다고 가정을 한다. 왜냐하면 Y 가 없이 X만 관측될 데이터이기 때문이다.\n",
    "X 만 관측되었다고 생각하고 이것을 모델에 넣어 결과값을 도출한 뒤, 실제로 있는 타겟값과 비교를 해서 실제 성능이 좋은지 아닌지를 판별하기 위해 소모되기 때문이다.\n",
    "그렇기 때문에 테스트셋에서는 성능이 안좋게 나올지라도 실제 학습데이터 에서는 성능이 높게 나올 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 위의 이슈는 Overfitting이 원인입니다.\n",
    "   - Overfitting은 모델이 학습데이터에 지나치게 적합화된 것을 의미합니다.\n",
    "   - Overfitting의 원인은 너무나 적은 데이터에 의해서도 일어날 수 있습니다.\n",
    "     - 10명에 대한 정보를 가지고 학습했다면 이 모델이 일반화 되었다기엔 많이 부족하겠지요?\n",
    "   - 또한 지나치게 편협한 데이터에 대해서도 Overfitting이 일어날 수 있습니다.\n",
    "     - 학습데이터의 연봉이 3000만원인 사람만 존재한다면 2000 혹은 4000만원인 사람이 입력으로 들어오게 되면 예측하기 힘들 것입니다.\n",
    "   - 혹은 데이터문제가 아닌 수식의 문제일 수 있습니다.\n",
    "   - 다양한 정규화 방법을 이용해 너무 복잡한 하거나 가중치가 너무 크게 잡힌 수식에 적용해 좀 더 단순한 회귀식으로 만들어 문제를 해결할 수 있을 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7~8.  유저 세분화 및 그룹핑을 위한 데이터 분석 작업\n",
    "\n",
    "### 배경 및 목표\n",
    "전체 소비자를 대상으로 한 마케팅 비용 및 리소스가 매우 큰 것으로 나타남에 따라, 전체 소비자를 세분화하여 그룹을 만든후 특정 그룹을 대상으로 마케팅을 진행하고자 한다. 분석가의 역할은 주어진 아래 데이터셋을 가지고 소비자를 세분화된 결과를 마케팅팀에 공유하는 것이다.\n",
    "\n",
    "### 데이터셋\n",
    "\n",
    "- 유저 아이디(숫자형)\n",
    "- 방문당 평균 결제횟수 (숫자형)\n",
    "- 방문당 공유 횟수 (숫자형)\n",
    "- 재방문율 (숫자형)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 지도학습(회귀), 지도학습(분류), 비지도학습, 강화학습 중에 어떤 모델을 적용할 것인가? 이유는 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "비지도학습을 적용해야하며 여기서 사용될 수 있는 X 는 방문당 평균 결제횟수, 방문당 공유 횟수, 재방문율이 될 수 있다.\n",
    "\n",
    "지도학습과는 다르게 Y 가 없는 X 만을 학습하는 방법이며, 입력데이터에 내제되어 있는 특성을 찾기위한 용도가 비지도학습이기 때문이다.\n",
    "\n",
    "이 문제에서는 4가지의 입력데이터의 내제되어 있는 특성을 사용하는것이기 때문에 비지도학습을 적용해야한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 만약 위 변수를 가지고 명확히 소비자가 세분화되지 않는다면 어떻게 해결하는 것이 좋을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "군집화(clustering) 를 사용하는 것이다.\n",
    "타겟 Y 가 없기때문에 X 들의 입력변수만으로 이 데이터에 가장 최적화된 구조를 찾는것이다. 이 때 각각의 클러스터들을 구분 할 수 있게되며 여기서 나온 X 들의 ID 를 얻을 수 있으며 이를 모델로 사용 할 수 있다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 새로운 변수를 데이터로부터 추출하던지 파생변수를 사용하던지 변수에 변화를 주어 문제를 해결할 수 있습니다.\n",
    "   - 다른 방법으로는 스케일이 다른 수치형 자료를 표준화점수로 변환하는 등 수치 변화해 보는 것이 도움이 될 수 있습니다.\n",
    "   - clustering에는 평균을 기준으로 각 개별값이 떨어져있는 정도를 계산한 후 표준편차로 나누는 z-score기법을 통한 표준화 방법을 많이 이용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. 머신러닝에서 손실함수는 모델의 학습과정에서 매우 중요한 역할을 한다. 이 역할에 대해 상세히 기술해보자. 그리고 회귀, 분류 각 문제별로 대표적인 손실함수를 예로 들어보자.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "학습 알고리즘이 작동하게끔 만드는 원동력이며 손실함수의 값을 줄여나가는 과정이 곧 모델을 학습하는 과정이다.\n",
    "\n",
    "여기서 손실이란 실제 데이터에서 관측된 결과와 모델에 의해 생성된 결과의 차이이다.\n",
    "손실이 작으면 작을수록 모델의 성능이 좋다.\n",
    "손실을 함수로 정의하는 이유는 수리적으로 계산하기 위해서이다.\n",
    "\n",
    "분류모델의 손실함수는 교차 엔트로피 회귀모델의 손실함수는 평균 제곱 오차가 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10~11. Gradient Descent의 과정과 SGD, MGD 처리 방식의 이해\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Gradient Descent 는 손실함수를 최소화하는 Weight을 찾아내기 위해 점진적으로 진행하는 최적화 방법중 하나이다. 경사하강법을 통해 손실함수 값을 최소화하는 과정을 간단히 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "loss function 의 값이 줄어들도록 weight 값을 조금씩 바꿔야한다.\n",
    "\n",
    "x 축을 weight 로 정의하고 y 축을 loss 라 정의했을 때 loss 를 w 로 미분하고, 미분값이 가리키는 방향의 반대방향으로 아주 조금씩 weight를 바꿔나가면 loss 를 감소시킬 수 있다.\n",
    "\n",
    "원하는 weight 를 찾기 위해서는 어떻게 하는가?\n",
    "\n",
    "아무것도 모르는 상태에서의 weight 의 초기값은 모르기때문에 random 한 값으로 시작한다.\n",
    "이를 random initialization 이라 한다.\n",
    "\n",
    "예시를 들자면 산 위에서 눈을 가리고 제일 낮은곳으로 가는 예를 들 수 있다.\n",
    "현재의 위치에서 내려가는 방향으로 천천히 내려가다보면 언젠가 제일 낮은점에 도착할 수 있다는 예시이다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 잘 알고 계시네요!\n",
    "   - 복습에 도움이ㅣ 될까 조금 더 덧붙여보았습니다ㅏ\n",
    "   - 일단 Loss Function을 정의했다면 이 함수를 미분한 값을 이용하여 Gradient Descent를 진행합니다.\n",
    "   - 이 때 Loss Function은 Convex 상태여야하고, Random하게 weight 값을 손실함수에 적용한 뒤 손실값을 확인합니다.\n",
    "   - 이 손실값을 최소로 하는, 즉 손실함수의 미분값(기울기)가 0인 weight를 점진적으로 찾는 것이 Gradient Descent의 목표입니다.\n",
    "   - 만약 기울기가 음수라면 x축 기준 오른쪽 방향으로 이동, 양수라면 왼쪽으로 이동하게 됩니다.\n",
    "     - 기울기는 항상 손실 함수의 값이 가장 크게 증가하는 방향을 향한다고 하셨는데 기울기는 단순히 그 값을 의미합니다. 때문에 기울기가 0인 weight를 weight update를 통해 찾아가는 것이지요.\n",
    "     - 최저점을 향해 가는 것이지 항상 음의 기울기를 사용하는 것은 아닙니다.\n",
    "     - convex 함수에서 weight 값의 변화를 살펴보면 기울기가 0인 weight를 기준으로 지그재그 형태로 움직일 수도 있습니다.\n",
    "   - 여기서 weight update는 현재 weight - 미분값 * learning rate가 됩니다..\n",
    "   - learning rate는 최저값에 다가가는 보폭을 의미하는데 이 값이 너무 작으면 수렴하는데 시간이 너무 오래 걸리고, 너무 크면 최저값에 도달하지 못하고 overshoot 하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 많은 데이터에 대해 한번에 Gradient Descent 를 적용했을 때(즉 Batch 처리) 학습에 시간이 오래 걸리는 문제가 발생한다. 따라서 Stochastic, Mini-batch Gradient Descent 과 같은 방법들을 사용하는데, 각 방법들에 대해 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stocahastic 는 데이터 중 data 중 1개만 뽑고 그 1개에 대해서 loss 를 계산하고 그 loss 가 전체의 loss 와 비슷할거라 가정하고 학습하며 weight 를 바꿔간다는 내용이다.\n",
    "\n",
    "mini-batch gradient descent 는 batch/stochastic 의 중간형태이며 data 중 n개를 뽑고 그 n개의 data에 대한 loss 를 계산하여 다 더한 뒤 이를 이용하여 미분해서 학습하겠다는 내용이다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 잘해주셨어요\n",
    "   - 조금 덧붙이자면\n",
    "   - minibatch는 n개의 loss 평균값을 이용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12~13. 오버피팅 개념 및 해결방법에 대한 이해\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 머신러닝 모델을 구축하기 위해 전체 데이터를 학습셋, 검증셋, 테스트셋으로 분리시키는데 이렇게 분리시키는 목은 무엇이며, 각 데이터셋의 이용 목적을 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "training set 은 학습에 사용하는 data 이고\n",
    "validation set 은 학습에 사용하지 않고, hyper parameter tuning 에 사용하는 data이다.\n",
    "test set 은 학습이 완전히 끝난 후에 model 을 평가하기 위한 data이다.\n",
    "\n",
    "이들을 분리시키는 목적은 모델의 성능을 잘 평가하기 위해서이다. \n",
    "test set 에서는 처음 보는 데이터에 대해서 평가를 해야하고 이러한 이유 때문에 training set, validation set, test set 을 분리한다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">- 리뷰\n",
    "   - 잘 알고 계시네요\n",
    "   - 조금 덧붙이자면\n",
    "   - 학습에 사용되지 않은 테스트셋 데이터를 이용해 일반화가 잘 이루어졌는지 파악합니다.\n",
    "   - 여기서 테스트셋은 모델이 오버피팅되지 않았는지에 대한 파악도 할 수 있도록 합니다.\n",
    "     - 일반화가 잘 되었다면 학습셋에서의 결과에서 크게 벗어나지 않을테니까요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.  과적합(Overftting)이 발생했을 때 이를 해결할 수 있는 방안은 무엇이 있을지 작성해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "첫번째로 data 의 양을 늘리는 방법이있다.\n",
    "\n",
    "둘째로는 regularization 방법이 있다.\n",
    "정규화라 하며 weight 값을 너무 크지않게 만들어주는 방법이며\n",
    "이에는 l1/l2 regularization, dropout, batch normaliaztion 이 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14~15. CNN 및  RNN 의 동작원리에 대한 이해와 활용\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 14. 주로 이미지 인식을 위해 가장 널리 알려진 CNN(Convolutional Neural Network)의 학습과정에 대해 간단히 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이미지 인식에 가장 많이 사용된다.\n",
    "일반적으로 convolution layer, pooling layer, fully-connected layer 로 구성되어 있으며 사진을 구분할 때 convolution layer 에서는 feature 를 추출하고 pooling layer 에서는 추출된 feature 들을 모아준다. fully-connected layer 에서는 최종적으로 모인 feature를 가지고 넣은 사진이 무엇인지 판단해준다.\n",
    "\n",
    "이미지를 각 tile 로 쪼개서 tile 의 특정 feature 를 추출하며 모든 tile 의 총합 점수를 계산하여 판단해낸다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. RNN(Recurrent Neural Network)의 개념과 대표적인 문제점, 해결방법에 대해 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sequence 가 길어지면 성능의 저하, 학습의 어려움, 오래전 입력된 값에 대해서는 잘 기억하지 못하는 문제점들이 존재한다.\n",
    "\n",
    "이를 해결하기 위해 만들어진 network 들은 lstm, gru 가 있다.\n",
    "lstm 은 long short term memory 이며\n",
    "gru 는 gated recurrent unit 이다.\n",
    "\n",
    "long term dependency 를 잘 해결하고 학습이 잘 되게 도와주는 역할을 한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
